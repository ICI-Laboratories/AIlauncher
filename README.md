# LMServ ‚Äì Servidor Ligero para LLMs Locales

> **Instalaci√≥n con un solo comando. Servidor con un solo comando. Cero dependencias de la nube.**

LMServ encapsula el rapid√≠simo backend de **`llama.cpp`** con una amigable CLI de **Typer** y un servidor de streaming **FastAPI**, permiti√©ndote ejecutar modelos de lenguaje modernos completamente en tu propio hardware, desde una laptop hasta un peque√±o servidor casero.

\<p align="center"\>
\<img src="docs/diagram.svg" width="680"
alt="Arquitectura: CLI ‚Üí FastAPI ‚Üí WorkerPool ‚Üí llama-cli" /\>
\</p\>

-----

## üî• Caracter√≠sticas Principales

| Caracter√≠stica | Descripci√≥n |
| :--- | :--- |
| **Instalador Multi-Distro** | Un script (`setup.sh`) detecta tu SO (Ubuntu, Fedora, Arch) y compila `llama.cpp` (CPU o CUDA). |
| **Uso de Herramientas (Agente)** | Define herramientas en JSON; el modelo puede decidir usarlas en un bucle de razonamiento. |
| **Gram√°ticas Forzadas (GBNF)** | Genera autom√°ticamente gram√°ticas para forzar al modelo a producir JSON v√°lido para las llamadas a herramientas. |
| **Workers Persistentes** | Reutiliza los procesos de `llama-cpp` para atender peticiones con muy baja latencia. |
| **Multi-Worker** | Capacidad para procesar m√∫ltiples peticiones de forma simult√°nea. |
| **Descarga Autom√°tica** | Descarga modelos de Hugging Face al primer uso. |

-----

## üöÄ Gu√≠a de Inicio R√°pido (Linux y WSL2)

### Paso 1: Instalaci√≥n

El script de instalaci√≥n automatizada se encarga de todo.
```bash
git clone https://github.com/ICI-Laboratories/AIlauncher.git
cd AIlauncher
bash setup.sh
```

### Paso 2: Uso B√°sico del Servidor

```bash
# Activa el entorno virtual
source env/bin/activate

# Define una clave de API para proteger tu servidor
export API_KEY=my-super-secret-key

# Lanza el servidor con un modelo de Hugging Face
lmserv serve --model ggml-org/gemma-3-1b-it-GGUF
```

### Paso 3: Uso con Herramientas (Modo Agente)

1.  **Crea un fichero de herramientas `tools.json`:**
    ```json
    {
      "tools": [
        {
          "name": "get_weather",
          "description": "Obtiene el clima actual para una ciudad.",
          "parameters": {
            "type": "object",
            "properties": {
              "city": { "type": "string", "description": "La ciudad, ej: 'San Francisco, CA'" },
              "unit": { "type": "string", "enum": ["celsius", "fahrenheit"] }
            },
            "required": ["city"]
          }
        }
      ]
    }
    ```
2.  **Lanza el servidor con el flag `--tools`:**
    ```bash
    lmserv serve \
      --model TheBloke/Mistral-7B-Instruct-v0.2-GGUF \
      --tools tools.json \
      --n-gpu-layers 35
    ```
3.  **Realiza una petici√≥n que requiera la herramienta:**
    ```bash
    curl -H "X-API-Key: my-super-secret-key" \
         -H "Content-Type: application/json" \
         -d '{"prompt":"¬øQu√© clima hace en Tokyo?"}' \
         http://localhost:8000/chat
    ```
    El servidor devolver√° la respuesta final del modelo despu√©s de haber llamado a la herramienta `get_weather`.

-----

## üõ†Ô∏è Gu√≠a de la API para Desarrolladores

### Flujo de Razonamiento y Herramientas

Cuando se usa el flag `--tools`, el endpoint `/chat` cambia su comportamiento:
1.  El servidor recibe el `prompt`.
2.  El modelo genera un JSON con su "pensamiento" (`thought`) y, opcionalmente, una llamada a una herramienta (`tool_call`).
3.  Si hay una `tool_call`, el servidor la ejecuta.
4.  El resultado de la herramienta se re-inserta en el contexto del modelo.
5.  El proceso se repite hasta que el modelo genera una respuesta final (un `thought` sin `tool_call`).
6.  La respuesta final (`thought`) se devuelve al cliente.

Este ciclo convierte a LMServ en un **agente b√°sico** capaz de usar herramientas para responder preguntas.

### Endpoint de Chat

*   **M√©todo:** `POST`
*   **Ruta:** `/chat`
*   **Cuerpo (JSON):** `{ "prompt": "Tu pregunta aqu√≠" }`
*   **Respuesta (`text/plain`):** La respuesta final del modelo despu√©s del ciclo de razonamiento.

-----

## ‚ú® Comandos de la CLI (`lmserv serve`)

| Par√°metro | Flag | Descripci√≥n | Panel |
| :--- | :--- | :--- | :--- |
| **Modelo** | `-m`, `--model` | **(Requerido)** ID de Hugging Face o ruta a un fichero `.gguf`. | Servidor |
| **Herramientas** | `--tools` | **(Opcional)** Ruta al fichero JSON que define las herramientas. | Servidor |
| **Workers** | `-w`, `--workers` | N√∫mero de workers a ejecutar en paralelo. | Servidor |
| **Host/Puerto** | `-H`, `-p` | Interfaz y puerto de red del servidor. | Servidor |
| **Capas en GPU** | `--n-gpu-layers` | N√∫mero de capas del modelo a descargar en la VRAM. | Modelo |
| **Tama√±o Contexto**| `--ctx-size` | Tama√±o del contexto del modelo en tokens. | Modelo |
| **Adaptador LoRA**| `--lora` | Ruta a un adaptador LoRA (`.gguf`) para aplicar al modelo. | Modelo |

</details>

\<details\>
\<summary\>\<code\>lmserv install llama\</code\> ‚Äì Compila el motor llama.cpp\</summary\>

| Flag | Descripci√≥n |
| :--- | :--- |
| `--output-dir PATH` | Directorio donde se clonar√° y compilar√° `llama.cpp`. Por defecto: `build/`. |
| `--cuda / --no-cuda` | Activa o desactiva la compilaci√≥n con soporte para GPU NVIDIA. |

\</details\>

-----

## üìú Licencia

Este proyecto se distribuye bajo la **Licencia MIT**.