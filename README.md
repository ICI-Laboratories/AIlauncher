# LMServ ‚Äì Lightweight Local LLM Service

> **One-command installer. One-command server. Zero cloud dependencies.**

LMServ wraps the blazing-fast **`llama.cpp`** backend with a friendly **Typer** CLI, a streaming **FastAPI** server, and optional **mDNS** discovery‚Äîso you can run modern LLMs entirely on your own hardware, from a laptop to a tiny homelab.

<p align="center">
  <img src="docs/diagram.svg" width="680"
       alt="Architecture: CLI ‚Üí FastAPI ‚Üí WorkerPool ‚Üí llama-cli" />
</p>

---

## üî• Highlights

| Feature | What it means |
|---------|---------------|
| **One-line installer** | `lmserv install llama` compiles `llama.cpp` *(CUDA or CPU)* and `lmserv install models ...` fetches GGUF weights with checksum guard. |
| **Streaming REST API** | `POST /chat` returns tokens **as they are generated** (ideal for websockets / SSE). |
| **Persistent WorkerPool** | Re-uses `llama-cli` processes for low-latency requests. |
| **LAN discovery** | `lmserv discover` lists other LMServ nodes via mDNS‚Äîperfect for offline clusters. |
| **Self-healing** | Crashed workers are respawned; health-check endpoint shows live capacity. |
| **Extensible** | Swap in other workers, add CLI sub-commands, or embed the Python API. |

---

## üìÅ Repository layout

```

lmserv/
‚îú‚îÄ‚îÄ discovery/     \# LAN announce & scan ‚Äì see discovery-README.md
‚îú‚îÄ‚îÄ install/       \# llama.cpp builder + model fetcher ‚Äì see install-README.md
‚îú‚îÄ‚îÄ server/        \# FastAPI app, WorkerPool, workers ‚Äì see server-README.md
‚îú‚îÄ‚îÄ cli.py         \# Typer CLI (serve / install / discover / ‚Ä¶)
‚îú‚îÄ‚îÄ config.py      \# Dataclass that hoists env vars & resolves paths
‚îî‚îÄ‚îÄ **init**.py    \# Version, `Config`, `run_cli()`

````

Each sub-folder ships its **own README** with deeper internals.

---

## üöÄ Quick start

> Works on Linux, macOS and Windows (PowerShell or *x64 Native Tools* prompt).

```bash
git clone [https://github.com/ICI-Laboratories/AIlauncher.git](https://github.com/ICI-Laboratories/AIlauncher.git)
cd AIlauncher

# 1) Python deps (editable install)
python -m venv env && source env/bin/activate         # or .\env\Scripts\activate on Windows
pip install -r requirements.txt
pip install -e .

# 2) Build llama.cpp with CUDA (omit --cuda for CPU-only)
lmserv install llama --output-dir build/ --cuda

# 3) Download a model from the catalog
lmserv install models gemma-2b --target-dir models/

# 4) Launch the API with 2 workers
export API_KEY=mysecret                               # Windows: set API_KEY=mysecret
lmserv serve -m models/gemma-2b-it-q4_0.gguf -w 2 -p 8000
````

Visit [http://localhost:8000/docs](https://www.google.com/search?q=http://localhost:8000/docs) for the autogenerated Swagger UI
or try:

```bash
curl -N -H "X-API-Key: mysecret" \
     -H "Content-Type: application/json" \
     -d '{"prompt":"Name a famous Peruvian dish"}' \
     http://localhost:8000/chat
```

-----

## üõ†Ô∏è CLI cheat-sheet

\<details\>
\<summary\>\<code\>lmserv serve\</code\> ‚Äì run the server\</summary\>

| Flag                    | Default        | Description                     |
| ----------------------- | -------------- | ------------------------------- |
| `-m, --model-path PATH` | ‚Äî *(required)* | GGUF model file.                |
| `-w, --workers INT`     | `2`            | Parallel `llama-cli` processes. |
| `-H, --host TEXT`       | `0.0.0.0`      | Bind address.                   |
| `-p, --port INT`        | `8000`         | HTTP port.                      |
| `--max-tokens INT`      | `128`          | Fallback if client omits it.    |
| `--llama-bin PATH`      | auto-detected  | Custom path to `llama-cli`.     |

\</details\>

\<details\>
\<summary\>\<code\>lmserv install llama\</code\> ‚Äì build llama.cpp\</summary\>

| Flag                    | Default  | Description             |
| ----------------------- | -------- | ----------------------- |
| `-o, --output-dir PATH` | `build/` | Clone + build location. |
| `--cuda / --no-cuda`    | `--cuda` | Toggle GPU support.     |

\</details\>

\<details\>
\<summary\>\<code\>lmserv install models\</code\> ‚Äì fetch GGUF models\</summary\>

| Arg / Flag              | Description                                                     |
| ----------------------- | --------------------------------------------------------------- |
| `NAMES‚Ä¶`                | Model IDs in the built-in catalog (`gemma-2b`, `phi3-mini`, ‚Ä¶). |
| `-d, --target-dir PATH` | Where to save them (`models/`).                                 |

\</details\>

Other helpers:

  * **`lmserv discover -t 5`** ‚Äì list LAN peers.
  * **`lmserv llama --help`** ‚Äì pass flags straight to `llama-cli`.
  * **`lmserv update`** ‚Äì `git pull` + `pip install -e .`.

-----

## üîë API endpoints

| Method & Path | Purpose                                       |
| ------------- | --------------------------------------------- |
| `GET /health` | Returns **`ok ‚Äì workers idle: N`**.           |
| `POST /chat`  | Stream LLM completions. Requires `X-API-Key`. |
| `GET /`       | Plain-text greeting.                          |

### `/chat` request body

```jsonc
{
  "prompt"        : "Explain CRDTs in 3 sentences.",
  "system_prompt" : "You are a distributed-systems tutor.",   // optional
  "max_tokens"    : 256,                                       // optional
  "temperature"   : 0.7,                                       // optional
  "top_p"         : 0.9,                                       // optional
  "repeat_penalty": 1.1                                        // optional
}
```

Response is **`text/plain`** streamed line-by-line (ideal for Server-Sent Events).

-----

## ‚öôÔ∏è Configuration via env vars

| Variable         | Default           | Effect                             |
| ---------------- | ----------------- | ---------------------------------- |
| `MODEL_PATH`     | models/gemma.gguf | Alias of `--model-path`.           |
| `WORKERS`        | 2                 | Parallel workers.                  |
| `HOST`, `PORT`   | 0.0.0.0, 8000     | Bind address.                      |
| `API_KEY`        | changeme          | Required header for `/chat`.       |
| `MAX_TOKENS`     | 128               | Fallback token limit.              |
| `LLAMA_BIN`      | *(auto)* | Path to `llama-cli`.               |
| `GPU_IDX`        | 0                 | Which GPU to load on.              |
| `LOG_LEVEL`      | INFO              | DEBUG for verbose output.          |
| `RATE_LIMIT_QPS` | 0                 | \>0 enables in-memory rate limiter. |

-----

## üß© Embedding LMServ as a library

```python
import asyncio
from lmserv import Config
from lmserv.server import WorkerPool

cfg = Config(model_path="models/gemma-2b-it-q4_0.gguf", workers=1)
pool = WorkerPool(cfg)

async def run():
    await pool.start()
    w = await pool.acquire()
    async for tok in w.infer("2+2="):
        print(tok, end="", flush=True)
    await pool.release(w)
    await pool.shutdown()

asyncio.run(run())
```

-----

## üèóÔ∏è Roadmap

  * WebSocket endpoint with JSON patches
  * On-disk cache for llama.cpp KV state
  * UI front-end (React / Next.js)
  * Hot-reloading model weights without downtime

-----

## ‚ú® Contributing

1.  Fork and create a feature branch.
2.  Follow the Quick-start to get a local dev environment.
3.  Run `pre-commit install` (lint/format hooks).
4.  Submit a PR‚ÄîCI will run unit tests + mypy + ruff.

-----

## üìú License

LMServ is released under the **MIT License**.
Third-party assets retain their respective licenses.

-----

**Happy self-hosting\! ü¶ô**
