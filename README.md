# LMServ ‚Äì Servidor Ligero para LLMs Locales

> **Instalaci√≥n con un solo comando. Servidor con un solo comando. Cero dependencias de la nube.**

LMServ encapsula el rapid√≠simo backend de **`llama.cpp`** con una amigable CLI de **Typer** y un servidor de streaming **FastAPI**, permiti√©ndote ejecutar modelos de lenguaje modernos completamente en tu propio hardware, desde una laptop hasta un peque√±o servidor casero.

\<p align="center"\>
\<img src="docs/diagram.svg" width="680"
alt="Arquitectura: CLI ‚Üí FastAPI ‚Üí WorkerPool ‚Üí llama-cli" /\>
\</p\>

-----

## üî• Caracter√≠sticas Principales

| Caracter√≠stica | Descripci√≥n |
| :--- | :--- |
| **Instalador Automatizado** | Un script (`setup.sh`) se encarga de compilar `llama.cpp` (para CPU o CUDA) y configurar todo el entorno. |
| **API de Streaming** | El endpoint `POST /chat` devuelve los tokens a medida que se generan, ideal para interfaces web en tiempo real. |
| **Workers Persistentes** | Reutiliza los procesos de `llama-cpp` para atender peticiones con muy baja latencia. |
| **Multi-Worker** | Capacidad para procesar m√∫ltiples peticiones de forma simult√°nea, una por cada worker. |
| **Auto-reparaci√≥n** | Los workers que fallan se reinician autom√°ticamente para asegurar la disponibilidad del servicio. |
| **Descarga Autom√°tica** | Utiliza la nueva funcionalidad de `llama.cpp` para descargar modelos directamente desde Hugging Face al primer uso. |

-----

## üöÄ Gu√≠a de Inicio R√°pido (Ubuntu)

La instalaci√≥n est√° completamente automatizada. Tras clonar el repositorio, solo necesitas ejecutar un comando.

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/ICI-Laboratories/AIlauncher.git
cd AIlauncher
```

### Paso 2: Ejecutar el Script de Instalaci√≥n

Este √∫nico comando se encargar√° de instalar las dependencias del sistema, configurar el entorno de Python, compilar `llama.cpp` y dejar todo listo.

```bash
bash setup.sh
```

*El script te pedir√° tu contrase√±a para instalar paquetes (`apt`) y te preguntar√° si deseas compilar con soporte para GPU si detecta una.*

-----

## üíª C√≥mo Usar el Servidor

Una vez finalizada la instalaci√≥n, sigue estos pasos para lanzar el servidor:

1.  **Activa el entorno virtual:**

    ```bash
    source env/bin/activate
    ```

2.  **Lanza el servidor con un modelo de Hugging Face:**

    ```bash
    export API_KEY=mysecret
    lmserv serve --model ggml-org/gemma-3-1b-it-GGUF --workers 2
    ```

    > La primera vez que uses un modelo, se descargar√° y guardar√° en cach√© autom√°ticamente. Esto puede tardar varios minutos. Los siguientes arranques ser√°n casi instant√°neos.

-----

## üõ†Ô∏è Gu√≠a de la API para Desarrolladores

Para integrar LMServ en tus aplicaciones, utiliza el siguiente endpoint.

### Endpoint de Chat

  * **M√©todo:** `POST`
  * **Ruta:** `/chat`
  * **URL Completa (ejemplo local):** `http://localhost:8000/chat`

### Cabeceras (Headers)

| Cabecera | Valor de Ejemplo | Obligatorio |
| :--- | :--- | :--- |
| `Content-Type` | `application/json` | **S√≠** |
| `X-API-Key` | `mysecret` | **S√≠** |

### Cuerpo de la Petici√≥n (JSON Body)

El cuerpo de la petici√≥n debe ser un objeto JSON. El √∫nico campo obligatorio es `prompt`.

| Par√°metro | Tipo | Obligatorio | Descripci√≥n |
| :--- | :--- | :--- | :--- |
| `prompt` | `string` | **S√≠** | El texto o la pregunta para el modelo. |
| `system_prompt` | `string` | No | Instrucci√≥n general para guiar el comportamiento del modelo (ej: "Eres un asistente servicial y creativo"). |
| `max_tokens` | `integer` | No | N√∫mero m√°ximo de tokens a generar en la respuesta. Por defecto: `128`. |
| `temperature` | `float` | No | Controla la creatividad. Un valor m√°s alto (ej. `0.8`) genera respuestas m√°s variadas. |
| `top_p` | `float` | No | M√©todo de muestreo alternativo a `temperature`. |
| `repeat_penalty`| `float` | No | Penaliza la repetici√≥n de palabras. Un valor com√∫n es `1.1`. |

### Formato de la Respuesta

La respuesta del servidor es un **flujo de texto plano** (`text/plain`), no un JSON. Los tokens se env√≠an uno por uno a medida que se generan, lo que permite mostrarlos en tiempo real en la aplicaci√≥n cliente.

### Ejemplos de Peticiones con `curl`

#### Petici√≥n Sencilla

```bash
curl -N -H "X-API-Key: mysecret" \
     -H "Content-Type: application/json" \
     -d '{"prompt":"Hola, ¬øqui√©n eres?"}' \
     http://localhost:8000/chat
```

#### Petici√≥n con Hiperpar√°metros

```bash
curl -N -H "X-API-Key: mysecret" \
     -H "Content-Type: application/json" \
     -d '{
       "prompt": "Escribe un poema corto sobre los limones de Tecom√°n.",
       "system_prompt": "Eres un poeta experto en la belleza de Colima.",
       "max_tokens": 100,
       "temperature": 0.75
     }' \
     http://localhost:8000/chat
```

-----

## ‚ú® Comandos de la CLI

\<details\>
\<summary\>\<code\>lmserv serve\</code\> ‚Äì Lanza el servidor de API\</summary\>

| Flag | Descripci√≥n |
| :--- | :--- |
| `-m, --model TEXT` | **(Requerido)** Ruta a un `.gguf` local o ID de un repositorio de Hugging Face. |
| `-w, --workers INT` | N√∫mero de procesos del modelo a ejecutar en paralelo. Por defecto: `2`. |
| `-H, --host TEXT` | Direcci√≥n de red en la que el servidor escuchar√°. Por defecto: `0.0.0.0`. |
| `-p, --port INT` | Puerto HTTP en el que se ejecutar√° el servidor. Por defecto: `8000`. |

\</details\>

\<details\>
\<summary\>\<code\>lmserv install llama\</code\> ‚Äì Compila el motor llama.cpp\</summary\>

| Flag | Descripci√≥n |
| :--- | :--- |
| `--output-dir PATH` | Directorio donde se clonar√° y compilar√° `llama.cpp`. Por defecto: `build/`. |
| `--cuda / --no-cuda` | Activa o desactiva la compilaci√≥n con soporte para GPU NVIDIA. |

\</details\>

-----

## üìú Licencia

Este proyecto se distribuye bajo la **Licencia MIT**.