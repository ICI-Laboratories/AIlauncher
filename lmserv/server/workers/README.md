# `server/workers` â€“ motores de inferencia

Cada sub-mÃ³dulo aquÃ­ implementa **un tipo de worker** que el
`WorkerPool` puede lanzar y reutilizar.  El contrato comÃºn
permite cambiar el backend sin tocar la capa HTTP:

```python
async def spawn()              -> None                    # levantar recurso
async def infer(prompt: str)   -> AsyncIterator[str]      # streaming tokens
async def stop()               -> None                    # liberar recurso
````

## Ãrbol actual

```
workers/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ llama.py        â† spawns llama-cli (referencia)
â”œâ”€â”€ cpp_bridge.py   â† bindings pybind11 (hot-paths)
â””â”€â”€ utils.py        â† _stream_reader y helpers compartidos
```

### 1. `llama.LlamaWorker`

* **Proceso externo**: `llama-cli -i --interactive-first`
* Lee stdout/stderr con `asyncio`, detecta *reverse-prompt* marker
  `<|LMSERV_USER_INPUT_START|>` para saber dÃ³nde termina la respuesta.
* â‰¤ 300 lÃ­neas para facilitar debug; algoritmos pesados â†’ C++.

### 2. `cpp_bridge`

* Carga `lmserv_cpp.*.so` si existe (compilado con pybind11).
  Implementa `sample_top_p` y `tokenize` en C++.
* Transparente: si la librerÃ­a no estÃ¡, cae en versiÃ³n Python.

### 3. `utils._stream_reader`

* Corrutina que empuja lÃ­neas de un `TextIO` a una `asyncio.Queue`.
  Detecta EOF y errores, avisando al *worker* vÃ­a `Event`.

## Como aÃ±adir un nuevo backend

1. Crea `workers/mlc.py` (por ejemplo) implementando la interfaz.
2. RegÃ­stralo opcionalmente en `workers.__init__` para exportarlo.
3. Ajusta `WorkerPool` si quieres mezclar distintos tipos en el mismo pool.

## Cambios recientes

| Fecha        | Autor       | DescripciÃ³n                             |
| ------------ | ----------- | --------------------------------------- |
| 2024-06-\*\* | @tu-usuario | ğŸš€ AÃ±adido `cpp_bridge` con stub Python |

## Pendiente

* [ ] Pool heterogÃ©neo (CPU/GPU mix).
* [ ] Supervisor que reinicie worker lento (< tokens/s umbral).
* [ ] IntegraciÃ³n con `vLLM` o `TensorRT-LLM`.

