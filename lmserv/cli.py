from __future__ import annotations

"""CLI para LMServ completamente corregido.

Principales cambios respecto a la versi√≥n anterior
--------------------------------------------------
* **No** se definen valores *default* dentro de `typer.Option` **cuando** esos
  defaults tambi√©n se pasan en la firma de la funci√≥n. As√≠ se evita el
  `TypeError: Option() got multiple values for argument 'default'`.
* Todas las opciones tienen flags expl√≠citos (`--workers`, `-w`, etc.) para que
  Click nunca confunda un valor no‚Äêcadena con un *param_decl*.
"""

import os
import subprocess
import sys
from pathlib import Path
from typing import Annotated, Optional

import typer

from .config import Config

###############################################################################
# CLI ra√≠z
###############################################################################

cli = typer.Typer(
    add_completion=False,
    help="CLI principal de LMServ. Usa ‚Äòlmserv <comando> --help‚Äô para detalles.",
    no_args_is_help=True,
)

###############################################################################
# Tipos reutilizables de opciones ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
###############################################################################

ModelPathOpt = Annotated[
    Path,
    typer.Option(
        "--model-path",
        "-m",
        exists=True,
        dir_okay=False,
        readable=True,
        help="Ruta al modelo .gguf que usar√°n los workers.",
    ),
]

WorkersOpt = Annotated[
    int,
    typer.Option(
        "--workers",
        "-w",
        min=1,
        show_default=True,
        help="N√∫mero de procesos llama-cli en paralelo.",
    ),
]

HostOpt = Annotated[
    str,
    typer.Option(
        "--host",
        "-H",
        help="Interfaz en la que escuchar√° FastAPI.",
    ),
]

PortOpt = Annotated[
    int,
    typer.Option(
        "--port",
        "-p",
        help="Puerto HTTP para el endpoint REST.",
    ),
]

MaxTokOpt = Annotated[
    int,
    typer.Option(
        "--max-tokens",
        help="L√≠mite de tokens a generar por petici√≥n.",
    ),
]

LLamaBinOpt = Annotated[
    Optional[Path],
    typer.Option(
        "--llama-bin",
        exists=True,
        dir_okay=False,
        writable=False,
        help="Ruta al ejecutable `llama-cli` (si no est√° en $PATH).",
    ),
]

###############################################################################
# Comando principal: serve
###############################################################################

@cli.command()
def serve(
    model_path: ModelPathOpt,
    workers: WorkersOpt = 2,
    host: HostOpt = "0.0.0.0",
    port: PortOpt = 8000,
    max_tokens: MaxTokOpt = 128,
    llama_bin: LLamaBinOpt = None,
) -> None:
    """Lanza la API REST y los *workers* de llama.cpp."""

    env = os.environ.copy()
    env.update(
        {
            "MODEL_PATH": str(model_path),
            "WORKERS": str(workers),
            "HOST": host,
            "PORT": str(port),
            "MAX_TOKENS": str(max_tokens),
        }
    )
    if llama_bin:
        env["LLAMA_BIN"] = str(llama_bin)

    typer.echo(f"üöÄ  Levantando LMServ: http://{host}:{port}/chat  (workers={workers})")
    subprocess.run(
        [
            sys.executable,
            "-m",
            "uvicorn",
            "lmserv.server.api:app",
            "--host",
            host,
            "--port",
            str(port),
        ],
        check=True,
        env=env,
    )

###############################################################################
# Sub-comandos de instalaci√≥n
###############################################################################

install_app = typer.Typer(
    help="Sub-comandos para compilar llama.cpp y descargar modelos .gguf.",
)
cli.add_typer(install_app, name="install")

OutputDirOpt = Annotated[
    Path,
    typer.Option(
        "--output-dir",
        "-o",
        help="Directorio destino donde quedar√° la build de llama.cpp.",
    ),
]

CudaOpt = Annotated[
    bool,
    typer.Option(
        "--cuda/--no-cuda",
        help="Compilar con soporte CUDA/cuBLAS.",
        rich_help_panel="Opciones de Build",
    ),
]

ModelNamesArg = Annotated[
    list[str],
    typer.Argument(
        ...,
        help="Nombre corto de modelos a bajar (p.e. ‚Äògemma-2b‚Äô, ‚Äòphi3-mini‚Äô).",
    ),
]

TargetDirOpt = Annotated[
    Path,
    typer.Option(
        "--target-dir",
        "-d",
        help="Carpeta destino de los .gguf.",
    ),
]

@install_app.command("llama")
def install_llama(
    output_dir: OutputDirOpt = Path("build/"),
    cuda: CudaOpt = True,
) -> None:
    """Compila llama.cpp con (o sin) soporte CUDA."""

    from .install.llama_build import build_llama_cpp

    build_llama_cpp(output_dir, cuda)
    typer.secho("‚úÖ  llama.cpp compilado correctamente.", fg=typer.colors.GREEN)

@install_app.command("models")
def install_models(
    names: ModelNamesArg,
    target_dir: TargetDirOpt = Path("models/"),
) -> None:
    """Descarga modelos .gguf predefinidos y los guarda en *target_dir*."""

    from .install.models_fetch import download_models

    download_models(names, target_dir)
    typer.secho("‚úÖ  Modelos descargados.", fg=typer.colors.GREEN)

###############################################################################
# Descubrimiento de nodos
###############################################################################

TimeoutOpt = Annotated[
    int,
    typer.Option(
        "--timeout",
        "-t",
        help="Segundos de b√∫squeda.",
    ),
]

@cli.command()
def discover(timeout: TimeoutOpt = 5) -> None:
    """Busca nodos LMServ v√≠a mDNS."""

    from .discovery.mdns import discover_nodes

    nodes = discover_nodes(timeout=timeout)
    if nodes:
        typer.secho("üåê  Nodos LMServ encontrados:", bold=True)
        for n in nodes:
            typer.echo(f" ‚Ä¢ {n.host}:{n.port} ‚Äì {n.info or 'sin descripci√≥n'}")
    else:
        typer.secho("üôÅ  No se detectaron nodos.", fg=typer.colors.YELLOW)

###############################################################################
# Passthrough a llama-cli
###############################################################################

@cli.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})
def llama(ctx: typer.Context) -> None:
    """Reenv√≠a cualquier argumento directamente a `llama-cli`."""

    cfg = Config()
    cmd = [str(cfg.llama_bin), *ctx.args]
    try:
        subprocess.run(cmd, check=True)
    except FileNotFoundError:
        typer.secho(
            "‚ùå  `llama-cli` no encontrado. Usa `lmserv install llama` o pasa --llama-bin.",
            fg=typer.colors.RED,
            err=True,
        )
        raise typer.Exit(1)

###############################################################################
# Actualizar repositorio
###############################################################################

@cli.command()
def update() -> None:
    """Hace `git pull` y reinstala el paquete en editable."""

    typer.echo("üîÑ  git pull origin main ‚Ä¶")
    subprocess.run(["git", "pull", "origin", "main"], check=True)
    typer.echo("üõ†Ô∏è   rebuild (si aplica) ‚Ä¶")
    subprocess.run([sys.executable, "-m", "pip", "install", "-e", "."], check=True)
    typer.secho("‚úÖ  Proyecto actualizado.", fg=typer.colors.GREEN)

###############################################################################
# Entry-point
###############################################################################

def _main() -> None:
    cli()

if __name__ == "__main__":
    _main()
